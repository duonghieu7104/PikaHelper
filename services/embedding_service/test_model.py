#!/usr/bin/env python3
"""
Test Vietnamese embedding model with sentence-transformers
"""

from sentence_transformers import SentenceTransformer
import torch
from pyvi import ViTokenizer

print("ğŸ‡»ğŸ‡³ Testing Vietnamese Embedding Model")
print("=" * 60)

# Download from the ğŸ¤— Hub
print("ğŸ“¥ Loading model: huyydangg/DEk21_hcmute_embedding")
model = SentenceTransformer("huyydangg/DEk21_hcmute_embedding")
print("âœ… Model loaded successfully")

# Define query (cÃ¢u há»i vá» PokeMMO) vÃ  docs (hÆ°á»›ng dáº«n)
query = "LÃ m tháº¿ nÃ o Ä‘á»ƒ táº£i game PokeMMO trÃªn Ä‘iá»‡n thoáº¡i?"
docs = [
    "HÆ°á»›ng dáº«n táº£i game PokeMMO cho Ä‘iá»‡n thoáº¡i Android vÃ  iOS",
    "CÃ¡ch hoÃ n thÃ nh cá»‘t truyá»‡n Pokemon Fire Red trong PokeMMO",
    "HÆ°á»›ng dáº«n kiáº¿m tiá»n hiá»‡u quáº£ báº±ng cÃ¡ch Rematch Elite 4",
    "XÃ¢y dá»±ng Ä‘á»™i hÃ¬nh PvP máº¡nh nháº¥t cho PokeMMO",
    "CÃ¡ch báº¯t Pokemon huyá»n thoáº¡i trong game PokeMMO"
]

print(f"\nğŸ“ Query: {query}")
print(f"ğŸ“š Number of documents: {len(docs)}")

# TÃ¡ch tá»« cho query
print("\nğŸ”§ Tokenizing query...")
segmented_query = ViTokenizer.tokenize(query)
print(f"   Original: {query}")
print(f"   Segmented: {segmented_query}")

# TÃ¡ch tá»« cho tá»«ng dÃ²ng vÄƒn báº£n
print("\nğŸ”§ Tokenizing documents...")
segmented_docs = [ViTokenizer.tokenize(doc) for doc in docs]
for i, (original, segmented) in enumerate(zip(docs, segmented_docs), 1):
    print(f"   Doc {i}:")
    print(f"      Original: {original}")
    print(f"      Segmented: {segmented}")

# Encode query and documents
print("\nğŸ§  Generating embeddings...")
query_embedding = model.encode([segmented_query])
doc_embeddings = model.encode(segmented_docs)

print(f"âœ… Query embedding shape: {query_embedding.shape}")
print(f"âœ… Document embeddings shape: {doc_embeddings.shape}")
print(f"ğŸ“Š Embedding dimension: {query_embedding.shape[1]}")

# Calculate cosine similarity
print("\nğŸ“ Calculating cosine similarity...")
similarities = torch.nn.functional.cosine_similarity(
    torch.tensor(query_embedding), torch.tensor(doc_embeddings)
).flatten()

# Sort documents by cosine similarity
sorted_indices = torch.argsort(similarities, descending=True)
sorted_docs = [docs[idx] for idx in sorted_indices]
sorted_scores = [similarities[idx].item() for idx in sorted_indices]

# Print sorted documents with their cosine scores
print("\nğŸ¯ Results (sorted by relevance):")
print("-" * 60)
for i, (doc, score) in enumerate(zip(sorted_docs, sorted_scores), 1):
    print(f"{i}. Score: {score:.4f}")
    print(f"   Document: {doc}")
    print()

# Test vá»›i má»™t cÃ¢u tiáº¿ng Viá»‡t Ä‘Æ¡n giáº£n
print("\nğŸ§ª Testing with simple Vietnamese sentence:")
test_sentence = "Xin chÃ o, tÃ´i lÃ  trá»£ lÃ½ AI chuyÃªn vá» hÆ°á»›ng dáº«n game PokeMMO"
print(f"   Original: {test_sentence}")

segmented_test = ViTokenizer.tokenize(test_sentence)
print(f"   Segmented: {segmented_test}")

test_embedding = model.encode([segmented_test])
print(f"   Embedding shape: {test_embedding.shape}")
print(f"   Embedding dimension: {test_embedding.shape[1]}")
print(f"   First 10 values: {test_embedding[0][:10]}")

# Check if embedding is not mock data
unique_values = len(set(test_embedding[0]))
print(f"   Unique values: {unique_values}")

if unique_values > 10:
    print("   âœ… Real embedding generated (not mock data)!")
else:
    print("   âš ï¸  Possible mock data detected")

print("\nğŸ‰ Test completed successfully!")

