#!/usr/bin/env python3
"""
RAG Query Engine - High Quality Results Only
Threshold: Documents > 0.6, Q&A > 0.7
"""

import os
import sys
import logging
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
import time

from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from sentence_transformers import SentenceTransformer
from pyvi import ViTokenizer
import google.generativeai as genai

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RAGQueryEngine:
    """High-quality RAG Query Engine with strict thresholds"""
    
    def __init__(self):
        # Qdrant configuration
        self.qdrant_host = os.getenv("QDRANT_HOST", "qdrant")
        self.qdrant_port = int(os.getenv("QDRANT_PORT", "6333"))
        
        # Gemini configuration
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not self.gemini_api_key:
            raise ValueError("GEMINI_API_KEY environment variable is required")
        
        # Initialize clients
        self.qdrant_client = QdrantClient(
            host=self.qdrant_host,
            port=self.qdrant_port
        )
        
        # Configure Gemini
        genai.configure(api_key=self.gemini_api_key)
        self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Load Vietnamese embedding model
        logger.info("üáªüá≥ Loading Vietnamese embedding model...")
        self.model = SentenceTransformer("huyydangg/DEk21_hcmute_embedding")
        logger.info("‚úÖ Vietnamese embedding model loaded successfully")
    
    def tokenize_text(self, text: str) -> str:
        """Tokenize Vietnamese text using pyvi"""
        try:
            return ViTokenizer.tokenize(text)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Tokenization failed for text: {text[:50]}... Error: {e}")
            return text
    
    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for Vietnamese text"""
        try:
            tokenized_text = self.tokenize_text(text)
            embedding = self.model.encode([tokenized_text])
            return embedding[0].tolist()
        except Exception as e:
            logger.error(f"‚ùå Failed to generate embedding: {e}")
            return None
    
    def search_documents(self, query: str, limit: int = 5, score_threshold: float = 0.6) -> List[Dict[str, Any]]:
        """Search in documents collection with high threshold"""
        logger.info(f"üîç Searching documents for: '{query}' (threshold: {score_threshold})")
        
        try:
            # Generate query embedding
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                return []
            
            # Search in documents collection
            results = self.qdrant_client.search(
                collection_name="documents_collection",
                query_vector=query_embedding,
                limit=limit,
                score_threshold=score_threshold,
                with_payload=True,
                with_vectors=False
            )
            
            logger.info(f"üìä Found {len(results)} document results (threshold: {score_threshold})")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Failed to search documents: {e}")
            return []
    
    def search_qa_pairs(self, query: str, limit: int = 5, score_threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Search in Q&A collection with high threshold"""
        logger.info(f"üîç Searching Q&A pairs for: '{query}' (threshold: {score_threshold})")
        
        try:
            # Generate query embedding
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                return []
            
            # Search in Q&A collection
            results = self.qdrant_client.search(
                collection_name="qa_collection",
                query_vector=query_embedding,
                limit=limit,
                score_threshold=score_threshold,
                with_payload=True,
                with_vectors=False
            )
            
            logger.info(f"üìä Found {len(results)} Q&A results (threshold: {score_threshold})")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Failed to search Q&A pairs: {e}")
            return []
    
    def format_high_quality_context(self, doc_results: List[Dict[str, Any]], qa_results: List[Dict[str, Any]]) -> str:
        """Format high-quality search results into context"""
        context_parts = []
        
        # Add document context (high quality only)
        if doc_results:
            context_parts.append("üìÑ **T√†i li·ªáu ch·∫•t l∆∞·ª£ng cao:**")
            for i, result in enumerate(doc_results, 1):
                payload = result.payload
                content = payload.get('content', '')
                file_name = payload.get('file_name', 'Unknown')
                score = result.score
                
                context_parts.append(f"{i}. **{file_name}** (ƒê·ªô li√™n quan: {score:.3f})")
                context_parts.append(f"   {content[:600]}...")
                context_parts.append("")
        
        # Add Q&A context (high quality only)
        if qa_results:
            context_parts.append("‚ùì **C√¢u h·ªèi v√† tr·∫£ l·ªùi ch·∫•t l∆∞·ª£ng cao:**")
            for i, result in enumerate(qa_results, 1):
                payload = result.payload
                question = payload.get('question', '')
                answer = payload.get('answer', '')
                score = result.score
                
                context_parts.append(f"{i}. **Q:** {question} (ƒê·ªô li√™n quan: {score:.3f})")
                context_parts.append(f"   **A:** {answer[:400]}...")
                context_parts.append("")
        
        return "\n".join(context_parts)
    
    def generate_high_quality_response(self, query: str, context: str) -> str:
        """Generate high-quality response using Gemini API"""
        logger.info("ü§ñ Generating high-quality response with Gemini...")
        
        if not context.strip():
            return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin ch·∫•t l∆∞·ª£ng cao cho c√¢u h·ªèi: '{query}'\n\nüí° **G·ª£i √Ω:** H√£y th·ª≠ di·ªÖn ƒë·∫°t c√¢u h·ªèi kh√°c ho·∫∑c s·ª≠ d·ª•ng t·ª´ kh√≥a c·ª• th·ªÉ h∆°n."
        
        try:
            # Create prompt for Gemini
            prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ PokeMMO, m·ªôt game Pokemon online. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:** {query}

**Th√¥ng tin tham kh·∫£o ch·∫•t l∆∞·ª£ng cao:**
{context}

**H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:**
1. Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
2. N·∫øu c√≥ th√¥ng tin kh√¥ng ch√≠nh x√°c, h√£y n√≥i r√µ
3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán v√† d·ªÖ hi·ªÉu
4. N·∫øu c·∫ßn th√™m th√¥ng tin, h√£y ƒë·ªÅ xu·∫•t ng∆∞·ªùi d√πng t√¨m hi·ªÉu th√™m
5. N·∫øu c√≥ link ho·∫∑c h√¨nh ·∫£nh li√™n quan, h√£y ƒë·ªÅ c·∫≠p ƒë·∫øn
6. ∆Øu ti√™n th√¥ng tin c√≥ ƒë·ªô li√™n quan cao (score > 0.6 cho docs, > 0.7 cho Q&A)

**Tr·∫£ l·ªùi:**
"""
            
            # Generate response with Gemini
            response = self.gemini_model.generate_content(prompt)
            
            if response.text:
                logger.info("‚úÖ High-quality response generated with Gemini")
                return response.text
            else:
                logger.warning("‚ö†Ô∏è Empty response from Gemini")
                return f"**C√¢u tr·∫£ l·ªùi ch·∫•t l∆∞·ª£ng cao cho:** '{query}'\n\n{context}"
                
        except Exception as e:
            logger.error(f"‚ùå Failed to generate response with Gemini: {e}")
            # Fallback to simple response
            return f"**C√¢u tr·∫£ l·ªùi ch·∫•t l∆∞·ª£ng cao cho:** '{query}'\n\n{context}"
    
    def query(self, user_query: str, max_docs: int = 3, max_qa: int = 3) -> Dict[str, Any]:
        """High-quality RAG query with strict thresholds"""
        logger.info(f"üöÄ Processing high-quality query: '{user_query}'")
        
        start_time = time.time()
        
        # Step 1: High-threshold vector search
        doc_results = self.search_documents(user_query, limit=max_docs, score_threshold=0.6)
        qa_results = self.search_qa_pairs(user_query, limit=max_qa, score_threshold=0.7)
        
        # Step 2: Format high-quality context
        context = self.format_high_quality_context(doc_results, qa_results)
        
        # Step 3: Generate high-quality response
        response = self.generate_high_quality_response(user_query, context)
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        # Prepare result
        result = {
            "query": user_query,
            "response": response,
            "context": {
                "documents": [
                    {
                        "file_name": r.payload.get('file_name', 'Unknown'),
                        "content": r.payload.get('content', '')[:300] + "...",
                        "score": r.score,
                        "source": "document",
                        "quality": "high" if r.score >= 0.6 else "medium"
                    }
                    for r in doc_results
                ],
                "qa_pairs": [
                    {
                        "question": r.payload.get('question', ''),
                        "answer": r.payload.get('answer', '')[:300] + "...",
                        "score": r.score,
                        "source": "qa",
                        "quality": "high" if r.score >= 0.7 else "medium"
                    }
                    for r in qa_results
                ]
            },
            "metadata": {
                "processing_time": round(processing_time, 2),
                "total_results": len(doc_results) + len(qa_results),
                "high_quality_results": len([r for r in doc_results if r.score >= 0.6]) + len([r for r in qa_results if r.score >= 0.7]),
                "thresholds": {
                    "documents": 0.6,
                    "qa_pairs": 0.7
                },
                "timestamp": datetime.now().isoformat()
            }
        }
        
        logger.info(f"‚úÖ High-quality query processed in {processing_time:.2f}s")
        logger.info(f"üìä Results: {len(doc_results)} docs + {len(qa_results)} Q&A = {len(doc_results) + len(qa_results)} total")
        return result
    
    def interactive_query(self):
        """Interactive high-quality query mode"""
        logger.info("ü§ñ High-Quality RAG Query Engine - Interactive Mode")
        logger.info("=" * 60)
        logger.info("üéØ Ch·∫ø ƒë·ªô ch·∫•t l∆∞·ª£ng cao:")
        logger.info("   üìÑ Documents: threshold > 0.6")
        logger.info("   ‚ùì Q&A: threshold > 0.7")
        logger.info("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (g√µ 'quit' ƒë·ªÉ tho√°t):")
        
        while True:
            try:
                user_input = input("\n‚ùì C√¢u h·ªèi: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    logger.info("üëã T·∫°m bi·ªát!")
                    break
                
                if not user_input:
                    continue
                
                # Process high-quality query
                result = self.query(user_input)
                
                # Display response
                print(f"\nü§ñ **Tr·∫£ l·ªùi ch·∫•t l∆∞·ª£ng cao:**")
                print(result['response'])
                
                # Display quality info
                print(f"\nüìä **Th√¥ng tin ch·∫•t l∆∞·ª£ng:**")
                print(f"- T√†i li·ªáu: {len(result['context']['documents'])} k·∫øt qu·∫£ (threshold: 0.6)")
                print(f"- Q&A: {len(result['context']['qa_pairs'])} k·∫øt qu·∫£ (threshold: 0.7)")
                print(f"- K·∫øt qu·∫£ ch·∫•t l∆∞·ª£ng cao: {result['metadata']['high_quality_results']}")
                print(f"- Th·ªùi gian x·ª≠ l√Ω: {result['metadata']['processing_time']}s")
                
            except KeyboardInterrupt:
                logger.info("\nüëã T·∫°m bi·ªát!")
                break
            except Exception as e:
                logger.error(f"‚ùå Error: {e}")
                print(f"‚ùå L·ªói: {e}")

def main():
    """Main function"""
    logger.info("ü§ñ High-Quality RAG Query Engine for PokeMMO")
    logger.info("=" * 60)
    
    try:
        # Initialize RAG engine
        rag_engine = RAGQueryEngine()
        
        # Check if running in interactive mode
        if len(sys.argv) > 1:
            # Single query mode
            query = " ".join(sys.argv[1:])
            result = rag_engine.query(query)
            
            print(f"ü§ñ **Tr·∫£ l·ªùi ch·∫•t l∆∞·ª£ng cao:**")
            print(result['response'])
            
            print(f"\nüìä **Th√¥ng tin ch·∫•t l∆∞·ª£ng:**")
            print(f"- T√†i li·ªáu: {len(result['context']['documents'])} k·∫øt qu·∫£ (threshold: 0.6)")
            print(f"- Q&A: {len(result['context']['qa_pairs'])} k·∫øt qu·∫£ (threshold: 0.7)")
            print(f"- K·∫øt qu·∫£ ch·∫•t l∆∞·ª£ng cao: {result['metadata']['high_quality_results']}")
            print(f"- Th·ªùi gian x·ª≠ l√Ω: {result['metadata']['processing_time']}s")
            
        else:
            # Interactive mode
            rag_engine.interactive_query()
            
    except Exception as e:
        logger.error(f"‚ùå System failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
